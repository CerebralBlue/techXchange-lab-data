{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use watsonx, Elasticsearch, and LangChain to answer questions (RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "#### About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Index knowledge base passages (once)\n",
    "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Data (test) loading](#data)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [Basic information how to connect to Elasticsearch](#elastic_conn)\n",
    "- **[Set up ElasticsearchStore (Langchain)](#elasticsearchstore)**\n",
    "    - [Embed and index documents with Elasticsearch](#elasticsearchstore_index)\n",
    "    - [Generate a retrieval-augmented response to a question](#predict)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "##  Set up the environment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"langchain==0.0.345\" | tail -n 1\n",
    "!pip install elasticsearch | tail -n 1\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu | tail -n 1\n",
    "!pip install sentence_transformers | tail -n 1\n",
    "!pip install humanize | tail -n 1\n",
    "!pip install pandas | tail -n 1\n",
    "!pip install rouge_score | tail -n 1\n",
    "!pip install nltk | tail -n 1\n",
    "!pip install wget | tail -n 1\n",
    "!pip install \"pydantic==1.10.0\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai>=0.1.2\" | tail -n 1\n",
    "!pip install ibm_watson_machine_learning | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "import pandas as pd\n",
    "import humanize\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the project id\n",
    "The API requires project id that provides the context for the call. The project id is a unique identifier of the project in which the model is deployed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Data (test) loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the test dataset. This dataset is used to calculate the metrics score for selected model, defined prompts and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "questions_test_filename = 'questions_test.csv'\n",
    "questions_train_filename = 'questions_train.csv'\n",
    "questions_test_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_test.csv'\n",
    "questions_train_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_train.csv'\n",
    "\n",
    "\n",
    "if not os.path.isfile(questions_test_filename): \n",
    "    wget.download(questions_test_url, out=questions_test_filename)\n",
    "\n",
    "\n",
    "if not os.path.isfile(questions_train_filename): \n",
    "    wget.download(questions_train_url, out=questions_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_test = './questions_test.csv'\n",
    "filename_train =  './questions_train.csv'\n",
    "\n",
    "test_data = pd.read_csv(filename_test)\n",
    "train_data = pd.read_csv(filename_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build up knowledge base\n",
    "\n",
    "The current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "We can generate dense vector representations using embedding models. In this notebook, we use <a href=\"https://www.sbert.net/\" target=\"_blank\" rel=\"noopener no referrer\">Sentence Transformers</a> <a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" target=\"_blank\" rel=\"noopener no referrer\">all-MiniLM-L6-v2</a> to embed both the knowledge base passages and user queries. `all-MiniLM-L6-v2` is a performant open-source model that is small enough to run locally.\n",
    "\n",
    "A vector database is optimized for dense vector indexing and retrieval. This notebook uses <a href=\"https://python.langchain.com/docs/integrations/vectorstores/elasticsearch#basic-example\" target=\"_blank\" rel=\"noopener no referrer\">Elasticsearch</a>, a distributed, RESTful search and analytics engine, capable of performing both vector and lexical search. It is built on top of the Apache Lucene library, which offers good speed and performance with all-MiniLM-L6-v2 embedding model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using is already split into self-contained passages that can be ingested by Elasticsearch. \n",
    "\n",
    "The size of each passage is limited by the embedding model's context window (which is 256 tokens for `all-MiniLM-L6-v2`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load knowledge base documents\n",
    "\n",
    "Load set of documents used further to build knowledge base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_dir = \"./knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = f\"{os.getcwd()}/knowledge_base\"\n",
    "if not os.path.isdir(my_path):\n",
    "   os.makedirs(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_filename = 'knowledge_base/psgs.tsv'\n",
    "documents_url = 'https://raw.githubusercontent.com/CerebralBlue/techXchange-lab-data/refs/heads/main/psgs.tsv'\n",
    "\n",
    "\n",
    "if not os.path.isfile(documents_filename): \n",
    "    wget.download(documents_url, out=documents_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.read_csv(f\"{knowledge_base_dir}/psgs.tsv\", sep='\\t', header=0)\n",
    "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by Elasticsearch. The performance of Elasticsearch may differ depending on the embedding model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "emb_func = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "## Foundation Models on watsonx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model\n",
    "You need to specify `model_id` that will be used for inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "model_id = ModelTypes.GRANITE_13B_INSTRUCT_V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model parameters\n",
    "We need to provide a set of model parameters that will influence the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: 'greedy',\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the `WatsonxLLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import WatsonxLLM\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id.value,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"elastic_conn\"></a>\n",
    "## Set up connectivity information to Elasticsearch\n",
    "\n",
    "**This notebook focuses on self-managed cluster using <a href=\"https://cloud.ibm.com/docs/databases-for-elasticsearch?topic=databases-for-elasticsearch-getting-started\" target=\"_blank\" rel=\"noopener no referrer\">IBM Cloud® Databases for Elasticsearch.</a>**\n",
    "\n",
    "The following cell retrieves the Elasticsearch users, password, host and port from the environment if available and prompts you otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    esuser = os.environ[\"ESUSER\"]\n",
    "except KeyError:\n",
    "    esuser = input(\"Please enter your Elasticsearch user name (hit enter): \")\n",
    "try:\n",
    "    espassword = os.environ[\"ESPASSWORD\"]\n",
    "except KeyError:\n",
    "    espassword = getpass.getpass(\"Please enter your Elasticsearch password (hit enter): \")\n",
    "try:\n",
    "    eshost = os.environ[\"ESHOST\"]\n",
    "except KeyError:\n",
    "    eshost = input(\"Please enter your Elasticsearch hostname (hit enter): \")\n",
    "try:\n",
    "    esport = os.environ[\"ESPORT\"]\n",
    "except KeyError:\n",
    "    esport = input(\"Please enter your Elasticsearch port number (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Elasticsearch will start with security features like authentication and TLS enabled. To connect to the Elasticsearch cluster you’ll need to configure the Python Elasticsearch client to use HTTPS with the generated CA certificate in order to make requests successfully. Details can be found <a href=\"https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/connecting.html#connect-self-managed-new\" target=\"_blank\" rel=\"noopener no referrer\">here</a>. In this notebook certificate fingerprints will be used for authentication. \n",
    "\n",
    "**Verifying HTTPS with certificate fingerprints (Python 3.11 or later)** If you don’t have access to the generated CA file from Elasticsearch you can use the following script to output the root CA fingerprint of the Elasticsearch instance with openssl s_client <a href=\"https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/connecting.html#_verifying_https_with_certificate_fingerprints_python_3_10_or_later\" target=\"_blank\" rel=\"noopener no referrer\"> (docs)</a>:\n",
    "\n",
    "\n",
    "The following cell retrieves the fingerprint information using a shell command and stores it in variable `ssl_assert_fingerprint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_ssl_fingerprint = !openssl s_client -connect $eshost:$esport  -showcerts </dev/null 2>/dev/null | openssl x509 -fingerprint -sha256 -noout -in /dev/stdin\n",
    "es_ssl_fingerprint = es_ssl_fingerprint[0].split(\"=\")[1]\n",
    "es_ssl_fingerprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"elasticsearchstore\"></a>\n",
    "## Set up ElasticsearchStore connector from Langchain\n",
    "\n",
    "\n",
    "We first create a regular Elasticsearch Python client connection. Then we pass it into LangChain's ElasticsearchStore wrapper together with the WatsonX model based embedding function.\n",
    "\n",
    "Consult the LangChain documentation For more information about <a href=\"https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html\" target=\"_blank\" rel=\"noopener no referrer\">ElasticsearchStore</a> connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "indexName = f\"langchain-test-{random.randint(0,2000)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_connection = Elasticsearch([f\"https://{esuser}:{espassword}@{eshost}:{esport}\"],\n",
    "                              basic_auth=(esuser, espassword),\n",
    "                              request_timeout=None,\n",
    "                              ssl_assert_fingerprint=es_ssl_fingerprint)\n",
    "\n",
    "knowledge_base = ElasticsearchStore(es_connection=es_connection,\n",
    "                                    index_name=indexName,\n",
    "                                    embedding=emb_func,\n",
    "                                    strategy=ElasticsearchStore.ApproxRetrievalStrategy(),\n",
    "                                    distance_strategy=\"DOT_PRODUCT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"elasticsearchstore_index\"></a>\n",
    "### Embed and index documents with Elasticsearch\n",
    "\n",
    "**Note: Could take several minutes if you don't have pre-built indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es_connection.indices.exists(index=indexName):\n",
    "    es_connection.indices.delete(index=indexName)\n",
    "_ = knowledge_base.add_texts(texts=documents.indextext.tolist(),\n",
    "                             metadatas=[{'title': title, 'id': doc_id}\n",
    "                                for (title, doc_id) in\n",
    "                                zip(documents.title, documents.id)],  # filter on these!\n",
    "                             index_name=indexName,\n",
    "                             ids=[str(i) for i in documents.id]  # unique for each doc\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look in Elasticsearch what the LangChain wrapper has created. First we display the newly created index (\"tables\" in Elasticsearch are always called \"index\"). Note the field `vector` of type `dense_vector` with `dot_product` similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(es_connection.indices.get(index=indexName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the number of documents loaded into the Elasticsearch index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = es_connection.count(index=indexName)[\"count\"]\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve a random document as a sample. Note the embedding in the vector field, that was generated with the WatsonX embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(es_connection.get(index=indexName, id=random.randint(0, len(documents)-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RetrievalQA` is a chain to do question answering.\n",
    "\n",
    "**Hint:** To use Chain interface from LangChain with watsonx.ai models you must call `model.to_langchain()` method. \n",
    "\n",
    "It returns `WatsonxLLM` wrapper compatible with LangChain CustomLLM specification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select questions\n",
    "\n",
    "The prompts we will use to test the RAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = {\n",
    "            'names of founding fathers of the united states?': \"Thomas Jefferson::James Madison::John Jay::George Washington::John Adams::Benjamin Franklin::Alexander Hamilton\",\n",
    "            'who played in the super bowl in 2013?': 'Baltimore Ravens::San Francisco 49ers'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve relevant context\n",
    "\n",
    "Fetch paragraphs similar to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=knowledge_base.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for question in questions_and_answers.keys():\n",
    "    result = qa({\"query\": question})\n",
    "    results.append(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the set of chunks for one of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results):\n",
    "    print(\"=========\")\n",
    "    print(\"Question = \", result['query'])\n",
    "    print(\"Answer = \", result['result'])\n",
    "    print(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", questions_and_answers[result['query']])\n",
    "    print(\"\\n\")\n",
    "    print(\"Source documents:\")\n",
    "    print(*(x.page_content for x in result['source_documents']), sep='\\n')\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
